{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa5f4da-58e4-4a48-9147-6f1128ac4042",
   "metadata": {},
   "source": [
    "# Using Kerchunk to improve NetCDF processing efficiency\n",
    "This notebook contains some example steps to build a `kerchunk` index file for a set of NetCDF files in the ECMWF ERA5 reanalysis data available as part of the AWS Public Dataset Program (https://registry.opendata.aws/ecmwf-era5/).\n",
    "\n",
    "## Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16620383-4cf5-4e9b-af40-0f4902826ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import boto3\n",
    "import botocore\n",
    "import fsspec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "#import hvplot.xarray\n",
    "import ujson\n",
    "import os\n",
    "import dask\n",
    "from dask.distributed import performance_report, Client, progress\n",
    "from pathlib import Path\n",
    "\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1787e-d3ee-4439-93c8-da9130d73438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kerchunk\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1f406-88c4-4895-978c-9fd6b723c0df",
   "metadata": {},
   "source": [
    "## ECS Cluster Initialisation\n",
    "This notebook expects Dask to be running in an ECS cluster.  There is an example AWS CloudFormation template available at https://github.com/awslabs/amazon-asdi/tree/main/examples/dask for quickly creating this environment in your own AWS account to run this notebook.\n",
    "\n",
    "**Update the variables below to identify the name of the ECS cluster in your environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b73d8-1ccf-4ab9-aca9-23c81b2815ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stackname=\"dask-environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93a2ba-a4fc-460d-90a1-9c1346867ddb",
   "metadata": {},
   "source": [
    "Identify the Dask scheduler and worker ECS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e30a53-d50e-4264-b3eb-cc3c5ff18a40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve stack outputs\n",
    "cfn = boto3.client('cloudformation')\n",
    "resp = cfn.describe_stacks(StackName=stackname)\n",
    "outputs = {}\n",
    "for output in resp['Stacks'][0]['Outputs']:\n",
    "    outputs[output['OutputKey']] = output['OutputValue']\n",
    "cluster = outputs['DaskECSClusterName']\n",
    "schedulerservice = outputs['DaskSchedulerServiceName']\n",
    "workerservice = outputs['DaskWorkerServiceName']\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c71d0-fd49-4eb1-a652-12281c8297a1",
   "metadata": {},
   "source": [
    "Start the Dask scheduler service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61217262-4e41-43b8-9a6d-308e3eebae83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecs = boto3.client('ecs')\n",
    "ecs.update_service(cluster=cluster, service=schedulerservice, desiredCount=1)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[schedulerservice])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924df731-d1ca-4c7b-b2ba-f5fb810e4fe2",
   "metadata": {},
   "source": [
    "The following will identify the public IP address of the Dask-Scheduler task (based on security group membership) and output the dashboard URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d800abce-db4a-42e8-9ef8-29dfad10b437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2')\n",
    "resp = ec2.describe_network_interfaces(\n",
    "  Filters=[{\n",
    "      'Name': 'group-id',\n",
    "      'Values': [outputs['DaskSchedulerSecurityGroup']]\n",
    "  }])\n",
    "schedulerurl = 'http://' + resp['NetworkInterfaces'][0]['Association']['PublicDnsName'] + '/status'\n",
    "from IPython.display import display,HTML\n",
    "display(HTML('Dask scheduler URL: <a href=\\'' + schedulerurl + '\\'>' + schedulerurl + '</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541bca09-75b1-41c8-b2fb-39fa2ddcc003",
   "metadata": {},
   "source": [
    "### Scale out Dask workers and connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760edc6b-12f2-4272-8e79-cfcf6fca6f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numWorkers=12\n",
    "ecs.update_service(cluster=cluster, service=workerservice, desiredCount=numWorkers)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[workerservice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c524c58-3c9e-4d55-aca4-6f9d1151121a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client('Dask-Scheduler.local-dask:8786')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51912a-a35e-4cb8-a0ac-5681f5cd25c6",
   "metadata": {},
   "source": [
    "Enable `fsspec` debugging if desired (this will increase the log output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d92a6d-5d67-4d22-a9a3-2c186d6371d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client.run(fsspec.utils.setup_logging, logger_name=\"fsspec\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c508c-8af1-4509-9a52-321cda22b209",
   "metadata": {},
   "source": [
    "## Build the Kerchunk Index\n",
    "\n",
    "We are now going to open a dataset locally and extract metadata into a JSON file using Kerchunk.  This step only needs to be done once!  After the index is created you can re-use it whenever processing the same dataset.  In this example we're going to build an index for a full year of the `air_temperature_at_2_metres` variable.\n",
    "\n",
    "First, create a list of files to target in our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52e2a6-985f-4ee3-94e3-86a049f92fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_year = 2020\n",
    "end_year = 2020\n",
    "years = list(np.arange(start_year, end_year+1, 1))\n",
    "months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "file_pattern = 'era5-pds/{year}/{month}/data/air_temperature_at_2_metres.nc'\n",
    "flist = [file_pattern.format(year=year, month=month) for year in years for month in months]\n",
    "flist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fae675e-eda1-4c35-bb2c-330fb6015d21",
   "metadata": {},
   "source": [
    "Create a local temporary folder to hold the index data for each NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3791d-bb74-4664-bc32-1d954333f686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_dir = 'jsons/'\n",
    "localfs = fsspec.filesystem('file')\n",
    "!rm -r jsons\n",
    "!mkdir jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e29dcb5-3932-46dc-b7ca-8d4bb87d123a",
   "metadata": {},
   "source": [
    "This code loops through each file and extract the metadata using the Kerchunk module, then write it to a local JSON file.  To start we're just defining the function, it is executed in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ae94a-a18d-4164-a6e3-73d5c60af724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "so = dict(mode='rb', default_fill_cache=False, default_cache_type='first')\n",
    "def gen_json(u):\n",
    "    with fs.open(u, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300, error=\"pdb\")\n",
    "        tchunks = h5chunks.translate()\n",
    "        # Also write to a file\n",
    "        parts = u.split('/')\n",
    "        year = parts[1]\n",
    "        month = parts[2]\n",
    "        fstem = Path(u).stem \n",
    "        outf = f'{json_dir}{year}{month}{fstem}.json'\n",
    "        print(outf)\n",
    "        with localfs.open(outf, 'wb') as f:\n",
    "            f.write(ujson.dumps(tchunks).encode());\n",
    "        return tchunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e958ddf-d606-4f65-a8e9-8d65e5754cce",
   "metadata": {},
   "source": [
    "The next step will build the index files - it will take some time so please be patient!  The code prints out each file as it is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e18b1c3-49eb-430f-ae9c-528b7f0d0dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "singles = []\n",
    "for f in flist:\n",
    "    singles.append(gen_json(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aea413-dd0d-489d-9e5e-d16e6ee9fb08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexfilelist = sorted(localfs.glob(f'{json_dir}*.json'))\n",
    "indexfilelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282d254-6357-403c-9830-a030836cbee3",
   "metadata": {},
   "source": [
    "Now that we have the index data, we can combine it into a single JSON file for the whole dataset using `MultiZarrToZarr`.  The below preprocessing step is necessary to add a default fill value otherwise Zarr will give us NaN co-ordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd08146-ffe2-4976-a5ad-cc4594e5c2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zarr\n",
    "def modify_fill_value(out):\n",
    "    out_ = zarr.open(out)\n",
    "    out_.lon.fill_value = -999\n",
    "    out_.lat.fill_value = -999\n",
    "    return out\n",
    "\n",
    "def postprocess(out):\n",
    "    out = modify_fill_value(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7ad65-6c4f-486f-93fd-8a10658ec98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mzz = MultiZarrToZarr(\n",
    "    indexfilelist,\n",
    "    remote_protocol='s3',\n",
    "    remote_options={'anon':True},\n",
    "    concat_dims=['time0'],\n",
    "    postprocess = postprocess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb444ef-b80f-4690-92f3-8ef6e1732829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "out = mzz.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157a917-e36e-40f5-8a1f-8f1f5422558d",
   "metadata": {},
   "source": [
    "We've got the combined index data in memory, now write it out to a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1fc72-878b-4764-a4af-d2dcf0e709b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "singleindexfile = f'era5-{start_year}-{end_year}.json'\n",
    "with localfs.open(singleindexfile, 'wb') as f:\n",
    "        f.write(ujson.dumps(out).encode());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99267c-b9d9-4d51-ad67-6df6ef008703",
   "metadata": {},
   "source": [
    "## Processing ERA5 Data Using a Kerchunk Index\n",
    "\n",
    "In the previous step we created a Kerchunk index on the ERA5 dataset for the year 2020 with a single variable.  Now we can use that index to open and process the dataset using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a846be-1b01-4257-8685-18a8eb031c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These should match your index file name created above\n",
    "start_year = 2020\n",
    "end_year = 2020\n",
    "index_file = f'era5-{start_year}-{end_year}.json'\n",
    "print(f'Loading index from {index_file}')\n",
    "\n",
    "with open(index_file) as f:\n",
    "    idx = ujson.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b4088-ad48-4866-8594-0908df2ac6e8",
   "metadata": {},
   "source": [
    "Create the S3 connection based on the Kerchunk index.  This is done using a `reference` type file system from the `fsspec` module, which is a specially created implementation for Kerchunk indexing: https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.implementations.reference.ReferenceFileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fe6c4-eb88-47df-85da-f66084521828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s_opts = {'skip_instance_cache':True}\n",
    "r_opts = {'anon':True}\n",
    "fs = fsspec.filesystem(\"reference\", fo=idx, ref_storage_args=s_opts,\n",
    "                       remote_protocol='s3', remote_options=r_opts)\n",
    "zarrmap = fs.get_mapper(\"\")\n",
    "list(zarrmap.keys())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc7606-32cb-4f47-841c-a4115460c4a4",
   "metadata": {},
   "source": [
    "Now open the dataset in xarray with the Zarr engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8a75d-cdc2-4baa-a685-f2494d954119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_dataset(zarrmap, engine=\"zarr\", backend_kwargs={'consolidated':False}, \n",
    "                     chunks={'time0':384})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0e510-4947-44bd-a821-03b55651f90e",
   "metadata": {},
   "source": [
    "Let's check that the air temperature dataset looks like what we expect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6f6f5-c19c-47b7-bfc7-bcbb2cd8bf29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.air_temperature_at_2_metres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fc052-1666-4fba-a038-b73b4e144fa2",
   "metadata": {},
   "source": [
    "## Optionally specify a region\n",
    "This reduces the amount of data we are working with by slicing to a specific region by lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be977a-b11d-45a4-b569-65d628af1805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dssubset = ds['air_temperature_at_2_metres'].sel(lat=slice(-10,-50),lon=slice(110,180)) - 273.15\n",
    "dssubset.attrs['units'] = 'C'\n",
    "dssubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286c5c4-6d23-434a-b68e-981a58caf54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset_mean = dssubset.mean(dim='time0')\n",
    "subset_mean = client.persist(subset_mean)\n",
    "progress(subset_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48e07f-a4a6-4c03-bd51-fddb924cdfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset_mean.compute()\n",
    "subset_mean.plot(figsize=(12,6), cmap='magma')\n",
    "plt.title(f'Mean 2-m Air Temperature {start_year} - {end_year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579d48c-7005-47ed-b66c-b03616f182bd",
   "metadata": {},
   "source": [
    "## Calculations on the global dataset\n",
    "The calculations below take us back to the global dataset which is held in the `ds` reference.\n",
    "\n",
    "### Convert units from K to C\n",
    "This performs a simple subtraction operation, to convert the temperature unit into Celcius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701bc98-adab-45bb-a0db-5517cdc71e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['air_temperature_at_2_metres'] = (ds.air_temperature_at_2_metres - 273.15)\n",
    "ds.air_temperature_at_2_metres.attrs['units'] = 'C'\n",
    "ds.air_temperature_at_2_metres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87745a9-85d5-428e-9a1f-de3f97e13fe1",
   "metadata": {},
   "source": [
    "Perform this calculation immediately using the dataset that is already loaded in worker memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdd396-3274-47f5-a10c-4be2cf6ccf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = client.persist(ds)\n",
    "progress(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf93c6e-53ae-48c6-a9c3-e3290a700abe",
   "metadata": {},
   "source": [
    "### Calculate the mean 2-m air temperature for all times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b92c20-f672-4af6-a6ea-1abbe0c6e125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculates the mean along the time dimension\n",
    "temp_mean = ds['air_temperature_at_2_metres'].mean(dim='time0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0914d-e45e-4dec-bf1f-41e830192918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b13311-9a41-424e-9635-17d26cdcdfb8",
   "metadata": {},
   "source": [
    "The expressions above didnâ€™t actually compute anything. They just build the dask task graph. To do the computations, we call the `persist` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462de7b4-d51f-44c1-beb7-e3c98fc3e5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean = temp_mean.persist()\n",
    "progress(temp_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea3d6c-de6b-4a94-ab77-95710835a77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean.compute()\n",
    "xpl = temp_mean.sortby('lon')\n",
    "xpl.plot(figsize=(30, 15))\n",
    "plt.title(f'{start_year} - {end_year} Mean 2-m Air Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03406706-2239-41e7-a358-2d2513e9f83f",
   "metadata": {},
   "source": [
    "## Dask Memory management\n",
    "\n",
    "Executing code in these cells can help you recover memory in the worker processes if things are getting tight.\n",
    "\n",
    "First, clear up all known datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337aadf1-eb4f-4982-9ab9-148702fec4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.cancel(ds)\n",
    "client.cancel(temp_mean)\n",
    "client.cancel(dssubset)\n",
    "client.cancel(subset_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52768a07-dd11-4ce7-b5c8-300a10093d51",
   "metadata": {},
   "source": [
    "This snippet of code reduces the workers memory footprint, which can be useful in debugging memory use.  It should get rid of most of the \"unmanaged\" memory reported in the dask dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3819439-0c98-4482-a0c2-ec09fdc4be1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "client.run(trim_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997573a6-0044-44eb-a4fe-c82402c2a615",
   "metadata": {},
   "source": [
    "If memory still isn't coming down, this is a last resort. It will terminate all workers and restart them fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a05824-69dd-427b-8fd1-f25a2b8bf4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defeb82-90a2-44d5-b4a5-b93ccb6f136e",
   "metadata": {},
   "source": [
    "## Cluster Scale Down\n",
    "\n",
    "When we are temporarily done with the cluster we can scale it down to save on costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab577e-7ce3-4349-bcf7-4ec6bfa4ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down workers\n",
    "ecs.update_service(cluster=cluster, service=workerservice, desiredCount=0)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[workerservice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e591ac7-e90c-4cf8-bc11-c5645cb1d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a02d0-181a-4f74-9f73-1f8f747719fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down scheduler\n",
    "ecs.update_service(cluster=cluster, service=schedulerservice, desiredCount=0)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[schedulerservice])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_daskpy3",
   "language": "python",
   "name": "daskpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
