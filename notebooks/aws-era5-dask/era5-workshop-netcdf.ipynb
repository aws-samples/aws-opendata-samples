{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing ERA5 data in NetCDF Format\n",
    "\n",
    "This notebook demonstrates how to work with the ECMWF ERA5 reanalysis available as part of the AWS Public Dataset Program (https://registry.opendata.aws/ecmwf-era5/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes Amazon SageMaker & AWS Fargate for providing an environment with a Jupyter notebook and Dask cluster. There is an example AWS CloudFormation template available at https://github.com/aws-samples/aws-opendata-samples/blob/main/projects/aws-era5-dask/dask-environment.yaml for quickly creating this environment in your own AWS account to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import boto3\n",
    "import botocore\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import fsspec\n",
    "import dask\n",
    "from dask.distributed import performance_report, Client, progress\n",
    "\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install extra software here, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install graphviz\n",
    "#import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Dask Client to talk to our Fargate Dask Distributed Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook expects Dask to be running in an ECS cluster.  There is an example AWS CloudFormation template available at https://github.com/aws-samples/aws-opendata-samples/blob/main/projects/aws-era5-dask/dask-environment.yaml for quickly creating this environment in your own AWS account to run this notebook.  The code in this notebook assumes you are running in this environment and will need adjusting if you are using a different Dask setup.\n",
    "\n",
    "**Update the stackname variable below to identify the name of your CloudFormation stack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stackname=\"dask-environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve stack outputs\n",
    "cfn = boto3.client('cloudformation')\n",
    "resp = cfn.describe_stacks(StackName=stackname)\n",
    "outputs = {}\n",
    "for output in resp['Stacks'][0]['Outputs']:\n",
    "    outputs[output['OutputKey']] = output['OutputValue']\n",
    "cluster = outputs['DaskECSClusterName']\n",
    "schedulerservice = outputs['DaskSchedulerServiceName']\n",
    "workerservice = outputs['DaskWorkerServiceName']\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the dask scheduler service through ECS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecs = boto3.client('ecs')\n",
    "ecs.update_service(cluster=cluster, service=schedulerservice, desiredCount=1)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[schedulerservice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will identify the public IP address of the Dask-Scheduler task (based on security group membership) and output the dashboard URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2')\n",
    "resp = ec2.describe_network_interfaces(\n",
    "  Filters=[{\n",
    "      'Name': 'group-id',\n",
    "      'Values': [outputs['DaskSchedulerSecurityGroup']]\n",
    "  }])\n",
    "schedulerurl = 'http://' + resp['NetworkInterfaces'][0]['Association']['PublicDnsName'] + '/status'\n",
    "from IPython.display import display,HTML\n",
    "display(HTML('Dask scheduler URL - click to open in new tab: <a href=\\'' + schedulerurl + '\\'>' + schedulerurl + '</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale out Dask Workers and connect\n",
    "Start the dask worker tasks and connect to the scheduler.  This will take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numWorkers=12\n",
    "ecs.update_service(cluster=cluster, service=workerservice, desiredCount=numWorkers)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[workerservice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client('dask-scheduler.local-dask:8786')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable fsspec debugging if desired.  This will increase the work log output but can be helpful to identify inefficient S3 access.  Worker log output can be found in CloudWatch Logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client.run(fsspec.utils.setup_logging, logger_name=\"fsspec\", level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open an Example File and Check the Native Chunking\n",
    "\n",
    "Before we start processing lets explore the dataset to discover its structure and chunking layout.  We want to chunk in an aligned way for maximum performance.\n",
    "\n",
    "First list the NetCDF files for a single month in the ERA5 public S3 bucket, using `s3fs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucketname=\"era5-workshop-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "fs.ls(f'{bucketname}/2021/01/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets open one of the files as a dataset using `xarray`, then explore its size, shape and chunk layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = f's3://{bucketname}/2021/05/data/air_temperature_at_2_metres.nc'\n",
    "ncfile = fsspec.open(url)\n",
    "ds = xr.open_dataset(ncfile.open())\n",
    "ds.air_temperature_at_2_metres.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note this causes the file to be read into memory\n",
    "# print('file size in GB {:0.2f}\\n'.format(ds.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the underlying HDF5 structure\n",
    "NetCDF version 4 uses HDF5 as the underlying file structure, so you can also use h5py directly to view information about the NetCDF data (assuming it is in netcdf4 / HDF5 format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File(fs.open(url))\n",
    "list(h5f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the chunk layout and size information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = h5f['air_temperature_at_2_metres']\n",
    "print(\"Number of chunks in file dataset:\", ds.id.get_num_chunks())\n",
    "print(\"Dataset shape:\", ds.shape)\n",
    "print(\"Chunk shape:\", ds.chunks)\n",
    "print(\"Compression:\", ds.compression)\n",
    "print(\"Dataset storage size: {:0.2f} MB\".format(ds.id.get_storage_size() / 1e6))\n",
    "print(\"Dataset full size: {:0.2f} MB\".format(ds.nbytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets have a look at the first few chunks to see how big they are on disk, and get a rough idea of the file layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "   print(ds.id.get_chunk_info(i))\n",
    "print(':')\n",
    "print(ds.id.get_chunk_info(ds.id.get_num_chunks()-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open 2-m air temperature as a single dataset\n",
    "This is where the real work begins.  We start by defining the set of S3 objects that we are going to process, using a file pattern.  We'll start with a full year of data - 12 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_year = 2021\n",
    "end_year = 2021\n",
    "years = list(np.arange(start_year, end_year+1, 1))\n",
    "months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "file_pattern = 's3://{bucket}/{year}/{month}/data/air_temperature_at_2_metres.nc'\n",
    "files_list = [file_pattern.format(bucket=bucketname,year=year,month=month) for year in years for month in months]\n",
    "files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "We now open each object as a \"file like object\" using s3fs, so we use them with xarray and dask.\n",
    "\n",
    "Depending on how the data is chunked, there are some parameters we can use to control `fsspec` caching that can improve performance.  By default `s3fs` uses `'bytes'` as the cache type, which is a read-ahead cache mechanism that will request and cache an additional 5MB for each chunk read from S3.  This turns out to be inefficient for the ERA5 dataset, because the NetCDF chunks are small (~200k) and not always stored sequentially.  \n",
    "\n",
    "The `default_block_size` parameter controls how much additional data will be requested from S3.  We've found values of between 256k and 512k to be optimal for the ERA5 dataset, but feel free to experiment.\n",
    "\n",
    "You can also experiment with other cache types, as implemented here: https://github.com/fsspec/filesystem_spec/blob/master/fsspec/caching.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll use our cluster to open files for the dataset in parallel across the workers\n",
    "@dask.delayed\n",
    "def s3open(path):\n",
    "    # Note the s3fs block size - this is the amount of data that will be read from s3 with each GetObject request.\n",
    "    # By default it is 5MB but for ERA5 we have found a 512k block yields more efficient S3 requests and faster performance.\n",
    "    fs = s3fs.S3FileSystem(default_fill_cache=False, default_block_size=512*1024)\n",
    "    return fs.open(path)\n",
    "\n",
    "files_mapper = [s3open(path) for path in files_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk sizing\n",
    "Now initialise the xarray dataset, specifying a chunk size that is a multiple of the underlying NetCDF chunk of 100/100/24. \n",
    "We are aiming for ~100MB chunk size as per dask recommendations.  Uncompressed chunks are 960KB so we set our chunks parameter to give us 128x that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(\n",
    "    files_mapper, \n",
    "    engine='h5netcdf', \n",
    "    chunks={'lon':400,'lat':200,'time0':384}, # 128x larger than underlying NetCDF chunk\n",
    "    concat_dim='time0', \n",
    "    combine='nested', \n",
    "    coords='minimal', \n",
    "    compat='override',\n",
    "    parallel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('ds size in GB {:0.2f}\\n'.format(ds.nbytes / 1e9))\n",
    "ds.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ds.info` output above shows us that there are four dimensions to the data: lat, lon, and time0; and two data variables: air_temperature_at_2_metres, and air_pressure_at_mean_sea_level.\n",
    "\n",
    "Let's check the chunking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.air_temperature_at_2_metres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert units to C from K\n",
    "This performs a simple subtraction operation, to convert the temperature unit into Celsius.  The operation will not actually be performed at this stage - not until we try to access the result or make the explicit call to `persist`, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['air_temperature_at_2_metres'] = (ds.air_temperature_at_2_metres - 273.15)\n",
    "ds.air_temperature_at_2_metres.attrs['units'] = 'C'\n",
    "ds.air_temperature_at_2_metres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all data into dask worker memory\n",
    "The following line reads the entire data set into worker memory.  This step is unnecessary at this stage, but it will make all subsequent calculations much faster (at the expense of memory usage!) and is a useful illustration of how dask works.  Otherwise, calculations are done without reading all data into worker memory at once, and data will need to be read back in for each calculation (taking much longer, but using less memory). \n",
    "\n",
    "The subtraction calculation we queued up above will also be executed during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = client.persist(ds)\n",
    "progress(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes data isn't evenly distributed, depending on the dataset and chunk size that we selected.  Here we rebalance the data across workers so that future tasks will make best use of cluster resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.rebalance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the mean 2-m air temperature for the entire dataset\n",
    "Now let's do some calculations across the entire data set, starting with calculation of the mean for every grid point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculates the mean along the time dimension\n",
    "temp_mean = ds['air_temperature_at_2_metres'].mean(dim='time0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expressions above didn’t actually compute anything. They just build the dask task graph. To do the computations, we call the `persist` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean = temp_mean.persist()\n",
    "progress(temp_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Average Surface Temperature\n",
    "To plot data, we need to read it back into the local notebook python environment.  This is done using the \"compute\" function.  Once the data is back in local memory, we can use matplotlib to display it visually.  For more information refer to: https://distributed.dask.org/en/latest/manage-computation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean.compute()\n",
    "temp_mean.plot(figsize=(30, 15))\n",
    "plt.title(f'Mean 2-m Air Temperature {start_year} - {end_year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats the mean of the hourly sample in the source dataset.  Let's down-sample the data by taking the daily maximum and re-calculating the mean based on that.  This is one line of code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_max_mean = ds['air_temperature_at_2_metres'].resample(indexer={\"time0\":'D'}).max().mean(dim='time0')\n",
    "daily_max_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't necessarily need to call `persist` here, the `compute` call below will trigger this for us - but this lets us see the progress in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_max_mean = daily_max_mean.persist()\n",
    "progress(daily_max_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_max_mean.compute()\n",
    "daily_max_mean.plot(figsize=(30, 15))\n",
    "plt.title(f'Average daily maximum temperature {start_year} - {end_year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for standard deviation\n",
    "The data is in memory so let's do another calculation - this time standard deviation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_std = ds['air_temperature_at_2_metres'].std(dim='time0')\n",
    "temp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_std = temp_std.persist()\n",
    "progress(temp_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_std.compute()\n",
    "temp_std.plot(figsize=(30, 15),cmap='inferno')\n",
    "plt.title(f'Standard Deviation 2-m Air Temperature {start_year} - {end_year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing a specific region\n",
    "This reduces the amount of data we are working with by slicing to a specific region by lat/lon\n",
    "\n",
    "To do this we use the `xarray.Dataset.sel` function and provide lat/lon slice co-ordinates.  The slice in the code below is for the ANZ region.  Feel free to adjust!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dssubset = ds['air_temperature_at_2_metres'].sel(lat=slice(0,-50),lon=slice(110,180))\n",
    "dssubset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of the region - this should be very fast because the data is in worker memory already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset_mean = dssubset.resample(indexer={'time0':'D'}).max().mean(dim='time0')\n",
    "subset_mean = client.persist(subset_mean)\n",
    "progress(subset_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subset_mean.compute()\n",
    "subset_mean.plot(figsize=(12,8), cmap='inferno')\n",
    "plt.title(f'ANZ Mean 2-m Air Temperature {start_year} - {end_year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot temperature time series for points\n",
    "This example creates a dataframe table of data for some specific locations defined in the array below - extracting time-series data for only the specific points we are interested in.\n",
    "\n",
    "Feel free to change the cities and locations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# location coordinates\n",
    "locs = [\n",
    "    {'name': 'Wellington', 'lon': 172.78, 'lat': -41.28},\n",
    "    {'name': 'Honolulu', 'lon': -157.84, 'lat': 21.29},\n",
    "    {'name': 'Seattle', 'lon': -122.33, 'lat': 47.61},\n",
    "    {'name': 'Melbourne', 'lon': 144.95, 'lat': -37.84}\n",
    "]\n",
    "\n",
    "# convert westward longitudes to degrees east\n",
    "for l in locs:\n",
    "    if l['lon'] < 0:\n",
    "        l['lon'] = 360 + l['lon']\n",
    "locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_locs = xr.Dataset()\n",
    "air_temp_ds = ds\n",
    "\n",
    "# interate through the locations and create a dataset\n",
    "# containing the temperature values for each location\n",
    "for l in locs:\n",
    "    name = l['name']\n",
    "    lon = l['lon']\n",
    "    lat = l['lat']\n",
    "    var_name = name\n",
    "\n",
    "    ds2 = air_temp_ds.sel(lon=lon, lat=lat, method='nearest')\n",
    "\n",
    "    lon_attr = '%s_lon' % name\n",
    "    lat_attr = '%s_lat' % name\n",
    "\n",
    "    ds2.attrs[lon_attr] = ds2.lon.values.tolist()\n",
    "    ds2.attrs[lat_attr] = ds2.lat.values.tolist()\n",
    "    ds2 = ds2.rename({'air_temperature_at_2_metres' : var_name}).drop(('lat', 'lon'))\n",
    "\n",
    "    ds_locs = xr.merge([ds_locs, ds2])\n",
    "\n",
    "ds_locs.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets extract the data - this should be fast because everything is in worker memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_locs = client.persist(ds_locs)\n",
    "progress(ds_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dataframe\n",
    "Conversion between an xarray DataArray into a pandas DataFrame (table) as time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_f = ds_locs.to_dataframe()\n",
    "df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_f.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_f.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot temperature timeseries\n",
    "\n",
    "We'll first re-sample the data from hourly to daily maximums.  Note the number of entries / size of the dataset is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rs = df_f.resample('D').max()\n",
    "rs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['lines.linewidth'] = 1.0\n",
    "matplotlib.rcParams['lines.linestyle'] = 'solid'\n",
    "ax = rs.plot(figsize=(30, 15), title=f\"ERA5 Daily Maximums {start_year} - {end_year}\", grid=1)\n",
    "ax.set(xlabel='Date', ylabel='2-m Air Temperature (deg C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Memory Management\n",
    "\n",
    "Executing code in these cells can help you recover memory in the worker processes if things are getting tight.\n",
    "\n",
    "Delete references to variables that might be held in Dask Worker memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.cancel(ds)\n",
    "client.cancel(temp_mean)\n",
    "client.cancel(subset_mean)\n",
    "client.cancel(temp_std)\n",
    "client.cancel(df_f)\n",
    "client.cancel(ds_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet of code reduces the workers memory footprint, which can be useful in debugging memory use. It should get rid of most of the \"unmanaged\" memory reported in the dask dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "client.run(trim_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In extreme cases you might want to restart Dask Workers - this will take a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster scale down\n",
    "\n",
    "When we are temporarily done with the cluster we can scale it down to save on costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecs.update_service(cluster=cluster, service=workerservice, desiredCount=0)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[workerservice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - stop the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecs.update_service(cluster=cluster, service=schedulerservice, desiredCount=0)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[schedulerservice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_daskpy3",
   "language": "python",
   "name": "daskpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
