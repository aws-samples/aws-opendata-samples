{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing ERA5 data in Zarr Format\n",
    "\n",
    "This notebook demonstrates how to work with the ECMWF ERA5 reanalysis available as part of the AWS Public Dataset Program (https://registry.opendata.aws/ecmwf-era5/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook utilizes Amazon SageMaker & AWS Fargate for providing an environment with a Jupyter notebook and Dask cluster. There is an example AWS CloudFormation template available at https://github.com/awslabs/amazon-asdi/tree/main/examples/dask for quickly creating this environment in your own AWS account to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import boto3\n",
    "import botocore\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import fsspec\n",
    "import dask\n",
    "from dask.distributed import performance_report, Client, progress\n",
    "\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install extra software here, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install graphviz\n",
    "#import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Dask Client to talk to our Fargate Dask Distributed Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook expects Dask to be running in an ECS cluster.  There is an example AWS CloudFormation template available at https://github.com/awslabs/amazon-asdi/tree/main/examples/dask for quickly creating this environment in your own AWS account to run this notebook.  The code in this notebook assumes you are running in this environment and will need adjusting if you are using a different Dask setup.\n",
    "\n",
    "**Update the stackname variable below to identify the name of your CloudFormation stack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stackname=\"dask-environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve details of the ECS cluster from the CloudFormation stack outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve stack outputs\n",
    "cfn = boto3.client('cloudformation')\n",
    "resp = cfn.describe_stacks(StackName=stackname)\n",
    "outputs = {}\n",
    "for output in resp['Stacks'][0]['Outputs']:\n",
    "    outputs[output['OutputKey']] = output['OutputValue']\n",
    "cluster = outputs['DaskECSClusterName']\n",
    "schedulerservice = outputs['DaskSchedulerServiceName']\n",
    "workerservice = outputs['DaskWorkerServiceName']\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the dask scheduler container through ECS and connect to it.  Note, the dashboard address displayed here is a private address that you won't be able to connect to - the public address is revealled in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ecs = boto3.client('ecs')\n",
    "ecs.update_service(cluster=cluster, service=schedulerservice, desiredCount=1)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[schedulerservice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will identify the public IP address of the Dask-Scheduler task (based on security group membership) and output the dashboard URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2')\n",
    "resp = ec2.describe_network_interfaces(\n",
    "  Filters=[{\n",
    "      'Name': 'group-id',\n",
    "      'Values': [outputs['DaskSchedulerSecurityGroup']]\n",
    "  }])\n",
    "schedulerurl = 'http://' + resp['NetworkInterfaces'][0]['Association']['PublicDnsName'] + '/status'\n",
    "from IPython.display import display,HTML\n",
    "display(HTML('Dask scheduler URL: <a href=\\'' + schedulerurl + '\\'>' + schedulerurl + '</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale out Dask Workers and connect\n",
    "Start the dask worker tasks and connect to the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numWorkers=12\n",
    "ecs.update_service(cluster=cluster, service=workerservice, desiredCount=numWorkers)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[workerservice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client('Dask-Scheduler.local-dask:8786')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open 2-m air temperature as a single dataset\n",
    "This is where the real work begins.  We start by defining the set of S3 objects that we are going to process, which is done using the dask s3fs module and a file pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_accum_var_dims(ds, var):\n",
    "    # Some varibles like precip have extra time bounds varibles, we drop them here to allow merging with other variables\n",
    "\n",
    "    # Select variable of interest (drops dims that are not linked to current variable)\n",
    "    ds = ds[[var]]\n",
    "\n",
    "    if var in ['air_temperature_at_2_metres',\n",
    "               'dew_point_temperature_at_2_metres',\n",
    "               'air_pressure_at_mean_sea_level',\n",
    "               'northward_wind_at_10_metres',\n",
    "               'eastward_wind_at_10_metres']:\n",
    "        ds = ds.rename({'time0':'valid_time_end_utc'})\n",
    "\n",
    "    elif var in ['precipitation_amount_1hour_Accumulation',\n",
    "                 'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation']:\n",
    "        ds = ds.rename({'time1':'valid_time_end_utc'})\n",
    "\n",
    "    else:\n",
    "        print(\"Warning, Haven't seen {var} varible yet! Time renaming might not work.\".format(var=var))\n",
    "\n",
    "    return ds\n",
    "\n",
    "@dask.delayed\n",
    "def s3open(path):\n",
    "    fs = s3fs.S3FileSystem(anon=True, default_fill_cache=False, \n",
    "                           config_kwargs = {'max_pool_connections': 20})\n",
    "    return s3fs.S3Map(path, s3=fs)\n",
    "\n",
    "def open_era5_range(start_year, end_year, variables):\n",
    "    ''' Opens ERA5 monthly Zarr files in S3, given a start and end year (all months loaded) and a list of variables'''\n",
    "\n",
    "    file_pattern = 'era5-pds/zarr/{year}/{month}/data/{var}.zarr/'\n",
    "    years = list(np.arange(start_year, end_year+1, 1))\n",
    "    months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "    l = []\n",
    "    for var in variables:\n",
    "        print(var)\n",
    "\n",
    "        # Get files\n",
    "        files_mapper = [s3open(file_pattern.format(year=year, month=month, var=var)) for year in years for month in months]\n",
    "\n",
    "        # Look up correct time dimension by variable name\n",
    "        if var in ['precipitation_amount_1hour_Accumulation']:\n",
    "            concat_dim='time1'\n",
    "        else:\n",
    "            concat_dim='time0'\n",
    "\n",
    "        # Lazy load\n",
    "        ds = xr.open_mfdataset(files_mapper, engine='zarr',\n",
    "                               concat_dim=concat_dim, combine='nested',\n",
    "                               coords='minimal', compat='override', parallel=True)\n",
    "\n",
    "        # Fix dimension names\n",
    "        ds = fix_accum_var_dims(ds, var)\n",
    "        l.append(ds)\n",
    "\n",
    "    ds_out = xr.merge(l)\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialise the xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "start_year = 2021\n",
    "end_year = 2021\n",
    "ds = open_era5_range(start_year, end_year, [\"air_temperature_at_2_metres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('ds size in GB {:0.2f}\\n'.format(ds.nbytes / 1e9))\n",
    "ds.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ds.info` output above shows us that there are four dimensions to the data: lat, lon, and time0; and two data variables: air_temperature_at_2_metres, and air_pressure_at_mean_sea_level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.air_temperature_at_2_metres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert units to C from K\n",
    "This performs a simple subtraction operation, to convert the temperature unit into Celcius. The operation will not actually be performed at this stage - not until we try to access the result or make the explicit call to `persist`, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['air_temperature_at_2_metres'] = (ds.air_temperature_at_2_metres - 273.15)\n",
    "ds.air_temperature_at_2_metres.attrs['units'] = 'C'\n",
    "ds.air_temperature_at_2_metres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all data into dask worker memory\n",
    "The following line reads the entire data set into worker memory.  This step makes subsequent calculations much faster and is a useful illustration of how dask works.  Otherwise, calculations are done without reading all data into worker memory at once, and data will need to be read back in for each calculation (taking much longer!).  \n",
    "\n",
    "The subtraction calculation we queued up above will also be executed during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = client.persist(ds)\n",
    "progress(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes data isn't evenly distributed, depending on the dataset and chunk size that we selected.  Here we rebalance the data across workers so that future tasks will make best use of cluster resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.rebalance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the mean 2-m air temperature for all times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculates the mean along the time dimension\n",
    "temp_mean = ds['air_temperature_at_2_metres'].mean(dim='valid_time_end_utc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expressions above didn’t actually compute anything. They just build the dask task graph. To do the computations, we call the `persist` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean = temp_mean.persist()\n",
    "progress(temp_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Average Surface Temperature\n",
    "To plot data, we need to read it back into the local notebook python environment.  This is done using the \"compute\" function.  Once the data is back in local memory, we can use matplotlib to display it visually.  For more information refer to: https://distributed.dask.org/en/latest/manage-computation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_mean.compute()\n",
    "temp_mean.plot(figsize=(30, 15))\n",
    "plt.title(f'Mean 2-m Air Temperature {start_year} - {end_year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats the mean of the hourly sample in the source dataset.  Let's down-sample the data by taking the daily maximum and re-calculating the mean based on that.  This is one line of code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_max_mean = ds['air_temperature_at_2_metres'].resample(indexer={\"valid_time_end_utc\":'D'}).max().mean(dim='valid_time_end_utc')\n",
    "daily_max_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't necessarily need to call `persist` here, the `compute` call below will trigger this for us - but this lets us see the progress in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_max_mean = daily_max_mean.persist()\n",
    "progress(daily_max_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "daily_max_mean.compute()\n",
    "daily_max_mean.plot(figsize=(30, 15))\n",
    "plt.title(f'Average daily maximum temperature {start_year} - {end_year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for standard deviation\n",
    "The data is in memory, so let's do another calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_std = ds['air_temperature_at_2_metres'].std(dim='valid_time_end_utc')\n",
    "temp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_std = temp_std.persist()\n",
    "progress(temp_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_std.compute()\n",
    "temp_std.plot(figsize=(30, 15), cmap='inferno')\n",
    "plt.title(f'Standard Deviation 2-m Air Temperature {start_year} - {end_year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot temperature time series for points\n",
    "This example creates a dataframe table of data for some specific locations defined in the array below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# location coordinates\n",
    "locs = [\n",
    "    {'name': 'Wellington', 'lon': 172.78, 'lat': -41.28},\n",
    "    {'name': 'Honolulu', 'lon': -157.84, 'lat': 21.29},\n",
    "    {'name': 'Seattle', 'lon': -122.33, 'lat': 47.61},\n",
    "    {'name': 'Melbourne', 'lon': 144.95, 'lat': -37.84}\n",
    "]\n",
    "\n",
    "# convert westward longitudes to degrees east\n",
    "for l in locs:\n",
    "    if l['lon'] < 0:\n",
    "        l['lon'] = 360 + l['lon']\n",
    "locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_locs = xr.Dataset()\n",
    "air_temp_ds = ds\n",
    "\n",
    "# interate through the locations and create a dataset\n",
    "# containing the temperature values for each location\n",
    "for l in locs:\n",
    "    name = l['name']\n",
    "    lon = l['lon']\n",
    "    lat = l['lat']\n",
    "    var_name = name\n",
    "\n",
    "    ds2 = air_temp_ds.sel(lon=lon, lat=lat, method='nearest')\n",
    "\n",
    "    lon_attr = '%s_lon' % name\n",
    "    lat_attr = '%s_lat' % name\n",
    "\n",
    "    ds2.attrs[lon_attr] = ds2.lon.values.tolist()\n",
    "    ds2.attrs[lat_attr] = ds2.lat.values.tolist()\n",
    "    ds2 = ds2.rename({'air_temperature_at_2_metres' : var_name}).drop(('lat', 'lon'))\n",
    "\n",
    "    ds_locs = xr.merge([ds_locs, ds2])\n",
    "\n",
    "ds_locs.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_locs = client.persist(ds_locs)\n",
    "progress(ds_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dataframe\n",
    "Conversion between an xarray DataArray into a pandas DataFrame (table) as time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_f = ds_locs.to_dataframe()\n",
    "df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_f.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_f.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot temperature timeseries\n",
    "We'll first re-sample the data from hourly to daily maximums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rs = df_f.resample('D').max()\n",
    "rs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['lines.linewidth'] = 1.0\n",
    "matplotlib.rcParams['lines.linestyle'] = 'solid'\n",
    "ax = rs.plot(figsize=(30, 15), title=f\"ERA5 Daily Maximums {start_year} - {end_year}\", grid=1)\n",
    "ax.set(xlabel='Date', ylabel='2-m Air Temperature (deg C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Memory management\n",
    "\n",
    "Executing code in these cells can help you recover memory in the worker processes if things are getting tight.\n",
    "\n",
    "First, clear up all known datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.cancel(ds)\n",
    "client.cancel(temp_mean)\n",
    "client.cancel(dssubset)\n",
    "client.cancel(subset_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet of code reduces the workers memory footprint, which can be useful in debugging memory use.  It should get rid of most of the \"unmanaged\" memory reported in the dask dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "client.run(trim_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If memory still isn't coming down, this is a last resort. It will terminate all workers and restart them fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster scale down\n",
    "\n",
    "When we are temporarily done with the cluster we can scale it down to save on costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numWorkers=0\n",
    "ecs.update_service(cluster=cluster, service=workerservice, desiredCount=numWorkers)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[workerservice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional - stop the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "ecs.update_service(cluster=cluster, service=schedulerservice, desiredCount=0)\n",
    "ecs.get_waiter('services_stable').wait(cluster=cluster, services=[schedulerservice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_daskpy3",
   "language": "python",
   "name": "daskpy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
